---
description: "Tried using Foundry Local and Ollama for local inference but felt slowed down at the time. I'd be keen to swap back to a local model given how the large providers are slowly catching down the subscription token limits."
pubDateTime: "2026-02-26T10:44:08.664Z"
daySummary: "Questioning SaaS longevity, swapping back to local models as cloud token limits tighten, agent governance bureaucracy, and remembering to run local inference on renewable energy."
tags: ["mastodon", "ai", "local-models"]
inReplyTo: "https://infosec.exchange/@tomgag/116136417545167228"
mastodonUrl: "https://fosstodon.org/@sealjay/116136519182812009"
---
